# What is Bayesâ€™ Theorem?

## Think of Bayesâ€™ Theorem as a way to update your belief about something when you get new evidence.
### 1. Old belief â†’ what you thought before (called prior probability)
### 2. New evidence â†’ something new you observe (called likelihood)
### 3. Updated belief â†’ what you now believe after seeing the evidence (called posterior probability)
## Simple formula: P(Yâˆ£X)= P(Xâˆ£Y)â‹…P(X) / P(Y)

## Where:
### 1. P(Yâˆ£X) â†’ probability of A given B (posterior)
### 2. P(Xâˆ£Y) â†’ probability of B given A (likelihood)
### 3. P(X) â†’ probability of A before seeing B (prior)
### 4. P(Y) â†’ total probability of B happening

## Bayesâ€™ Theorem is the foundation for probabilistic ML models.

### For example: Spam Email Detection:
### 1. Prior: probability of an email being spam
### 2. Evidence: email contains â€œWin moneyâ€
### 3. Posterior: probability that this email is spam given these words.

## NOTE: Bayesâ€™ Theorem combines the prior belief and the likelihood of observed data to compute the posterior probability.
## NOTE: Bayesâ€™ Theorem is just a math rule for probability.

# What is Naive's Bayes?

## Naive Bayes is a simple but powerful ML algorithm based on Bayesâ€™ Theorem.
## â€œNaiveâ€ â†’ assumes that all features are independent (this is a simplification, often not true, but it works surprisingly well).
## â€œBayesâ€ â†’ uses Bayesâ€™ Theorem to calculate probabilities.
## Formula for classification:
## P(Classâˆ£Features)= P(Featuresâˆ£Class)â‹…P(Class) / P(Features)
## Where:
## P(Classâˆ£Features) â†’ probability of the class (like spam) given features (like words in email)
## P(Featuresâˆ£Class) â†’ likelihood of features given the class
## P(Class) â†’ prior probability of the class
## P(Features) â†’ probability of observing these features (normalizing factor)

# How it works in simple steps:
## Imagine spam detection:
## Collect data: emails labeled spam or not spam.
## Count how often words appear in spam vs non-spam.
## For a new email, calculate probability it is spam using Bayes theorem for every word.
## Pick the class with highest probability â†’ classify email as spam or not.

# NOTE: In Naive Bayes, we almost always ignore the denominator ğ‘ƒ(Features). Hereâ€™s why, explained clearly:

## Naive Bayes formula
## P(Classâˆ£Features)= P(Featuresâˆ£Class)â‹…P(Class) / P(Features) 
## Here, denominator is P(Features), we can ignore it as it's same for all classes of Y.

## Key point
## 1. We donâ€™t need exact probabilities, only which class has the highest probability.
## 2. Ignoring P(Features) makes computation easier and it doesnâ€™t affect the result.

